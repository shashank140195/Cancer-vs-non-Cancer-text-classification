{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "633b7162-bec5-4fe4-b28d-64cc1c80bad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the training Data you want to train. Enter either train 1 or train 2. Your answer:  train 1\n",
      "Do you want to Binarise the document? Yes/No. Your answer:  no\n",
      "Do you want to exclude stop words? Yes/No. Your answer:  no\n"
     ]
    }
   ],
   "source": [
    "#importing packages\n",
    "import pandas as pd\n",
    "import math\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "\n",
    "#getting input from the user\n",
    "training_dataset_name = input(\"Enter the training Data you want to train. Enter either train 1 or train 2. Your answer: \")\n",
    "binary_flag = input(\"Do you want to Binarise the document? Yes/No. Your answer: \")\n",
    "stop_word_exclusion_flag = input(\"Do you want to exclude stop words? Yes/No. Your answer: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5de1101-640b-40b9-94a6-07ff43432001",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiialising training file name\n",
    "training_file_name = ''\n",
    "\n",
    "#getting path of current directory \n",
    "current_directory = os.getcwd()\n",
    "\n",
    "#naming the filenames\n",
    "if training_dataset_name.lower() == 'train 1':\n",
    "    training_file_name = (current_directory + (\"/training1_v1.csv\"))\n",
    "    \n",
    "elif training_dataset_name.lower() == 'train 2':\n",
    "    training_file_name = (current_directory + (\"/training2_v1.csv\"))\n",
    "    \n",
    "else :\n",
    "    raise Exception(\"Sorry, you entered incorrect value, Run again and enter either train 1 or train 2\")\n",
    "       \n",
    "testing_file_name = (current_directory + (\"/testing1_v1.csv\"))\n",
    "stop_words_file_name = (current_directory + (\"/StopWordList.csv\"))\n",
    "\n",
    "#reading the files\n",
    "training_dataset = pd.read_csv(training_file_name)\n",
    "testing_dataset = pd.read_csv(testing_file_name)\n",
    "stop_words_dataset = pd.read_csv(stop_words_file_name)\n",
    "\n",
    "#creating a list to store all stop words from the given document\n",
    "stop_word_list = []\n",
    "if stop_word_exclusion_flag.lower() == 'yes':\n",
    "    for i in stop_words_dataset.iterrows():\n",
    "        stop_word_list.append(i[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fea0522d-b456-4b45-abe3-311ea97de547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to make operations on a document\n",
    "def get_document_info(iterator):\n",
    "    document = iterator[1][0]\n",
    "    document = document.lower()\n",
    "    document_class = iterator[1][1]\n",
    "    \n",
    "    # Binarise the document if user input is yes\n",
    "    if binary_flag.lower() == 'yes':\n",
    "        lists = document.split()\n",
    "        document = ''\n",
    "        binarized = []\n",
    "        [binarized.append(x) for x in lists if x not in binarized]\n",
    "        document = ' '.join([str(elem) for elem in binarized])\n",
    "        \n",
    "    #exclude stop words if user input is yes\n",
    "    if stop_word_exclusion_flag.lower() == 'yes':\n",
    "        slists = document.split()\n",
    "        document_without_stop_words = [t for t in slists if t not in stop_word_list]\n",
    "        document = ''\n",
    "        document = ' '.join([str(elements) for elements in document_without_stop_words])\n",
    "    \n",
    "    #Removing the given special characters from the document\n",
    "    tokenized_document = re.split(r'[\\n\\t\\s.,;:''\"\"()?!]', document)\n",
    "    tokenized_document = [i for i in tokenized_document if i]\n",
    "        \n",
    "    return document_class, tokenized_document\n",
    "\n",
    "#method to create bag of words\n",
    "def create_bag_of_words(training_dataset):\n",
    "    bag_of_words_positive = []  #All words in positive (Include duplicates)\n",
    "    bag_of_words_negative = []  #All words in Negative (Include duplicates)\n",
    "    \n",
    "    #looping through the each document in training set\n",
    "    for i in training_dataset.iterrows():\n",
    "        document_class, tokenized_document = get_document_info(i)\n",
    "        \n",
    "        if document_class.lower() == 'cancer':\n",
    "            \n",
    "        # loop till words are present in list tokenized_document\n",
    "            for i in tokenized_document:\n",
    "                \n",
    "                # insert value in BagOfWordsPositive\n",
    "                bag_of_words_positive.append(i) \n",
    "        \n",
    "        else:\n",
    "            for i in tokenized_document:\n",
    "                \n",
    "                # insert value in BagOfWordsNegative\n",
    "                 bag_of_words_negative.append(i) \n",
    "                \n",
    "    return bag_of_words_positive, bag_of_words_negative\n",
    "\n",
    "#calling the method to create bag of words\n",
    "bag_of_words_can, bag_of_words_non_can = create_bag_of_words(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cc682db-f418-41c9-a832-4afb240470e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary storing Frequency of each word in positive class\n",
    "freq_positive_class = {}\n",
    "\n",
    "#Dictionary storing Frequency of each word in negative class\n",
    "freq_negative_class = {} \n",
    "\n",
    "#removing duplicate words\n",
    "unique_positive_class_words= set(bag_of_words_can)\n",
    "unique_negative_class_words= set(bag_of_words_non_can)\n",
    "\n",
    "#method to create a list of vocabulory\n",
    "def create_vocabulary (uniquePositives, uniqueNegatives):\n",
    "    union = set.union(uniquePositives, uniqueNegatives)\n",
    "    \n",
    "    return list(union)\n",
    "\n",
    "#method to count the frequency of each word\n",
    "def calculate_frequency (uniquePositives, uniqueNegatives):\n",
    "    \n",
    "    for x in uniquePositives:\n",
    "        freq_positive_class[x] = bag_of_words_can.count(x)\n",
    "        \n",
    "    for y in uniqueNegatives:  \n",
    "        freq_negative_class[y] = bag_of_words_non_can.count(y)\n",
    "        \n",
    "#Calling the functions to get vocabulary and frequency        \n",
    "vocabulary = create_vocabulary(unique_positive_class_words, unique_negative_class_words)\n",
    "calculate_frequency(unique_positive_class_words, unique_negative_class_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "947656a3-dd43-4cff-b235-449dbbd5b409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set used   : train 1\n",
      "Stop words excluded : no\n",
      "Binary version      : no\n",
      "true positives      : 74\n",
      "false negatives     : 26\n",
      "false positives     : 36\n",
      "Precision is       : 0.67\n",
      "Recall is          : 0.74\n",
      "F-score is         : 0.7\n"
     ]
    }
   ],
   "source": [
    "#method top calculate the perdormance matrix\n",
    "def calculate_scores(testing_dataset, training_dataset):\n",
    "    \n",
    "    # Total number of positive and negative docs\n",
    "    count_positive_doc = len(training_dataset[training_dataset['class'] == 'cancer'])\n",
    "    count_negative_doc = len(training_dataset[training_dataset['class'] == 'nocancer'])\n",
    "\n",
    "    #probability of each class\n",
    "    prob_positive_class =  count_positive_doc/ (count_positive_doc + count_negative_doc)\n",
    "    prob_negative_class =  count_negative_doc/ (count_positive_doc + count_negative_doc)\n",
    "    \n",
    "    #tp = true positive, fp = false positive, fn = false negative\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    #loop through the documents in testing dataset\n",
    "    for i in testing_dataset.iterrows():\n",
    "        document = i[1][0]\n",
    "        document = document.lower()\n",
    "        document_class = i[1][1]\n",
    "        tokenized_document = re.split(r'[\\n\\t\\s.,;:''\"\"()?!]', document)\n",
    "        tokenized_document = [i for i in tokenized_document if i]\n",
    "        \n",
    "        #initialising the value with the log of probability of each class calculated above\n",
    "        sum_positive = math.log(prob_positive_class)\n",
    "        sum_negative = math.log(prob_negative_class)\n",
    "        \n",
    "        #looping through the each word of test document\n",
    "        for j in tokenized_document:\n",
    "            \n",
    "            #checking if word is present in vocabulary and if not present we will ignore the token\n",
    "            if j in vocabulary:\n",
    "                freq_in_positive_class = 0\n",
    "                freq_in_negative_class = 0\n",
    "                \n",
    "                #finding the frequency of the word in positive docs\n",
    "                if j in freq_positive_class.keys():\n",
    "                    freq_in_positive_class = freq_positive_class[j]\n",
    "                else:\n",
    "                    freq_in_positive_class = 0\n",
    "                \n",
    "                #calculating the total probability of the word in positive docs\n",
    "                probab_positive = math.log((freq_in_positive_class + 1)/ (len(vocabulary) + len(bag_of_words_can)))\n",
    "                sum_positive =  probab_positive +  sum_positive\n",
    "                \n",
    "                #finding the frequency of the word in negative docs\n",
    "                if j in freq_negative_class.keys():\n",
    "                    freq_in_negative_class = freq_negative_class[j]\n",
    "                else:\n",
    "                    freq_in_negative_class = 0\n",
    "                    \n",
    "                #calculating the total probability of the word in the negative docs\n",
    "                probab_negative = math.log((freq_in_negative_class + 1)/ (len(vocabulary) + len(bag_of_words_non_can)))\n",
    "                sum_negative =  sum_negative +  probab_negative\n",
    "                \n",
    "        #comparing the values \n",
    "        if sum_positive > sum_negative:\n",
    "            predicted_class = 'cancer'\n",
    "            \n",
    "        else:\n",
    "            predicted_class = 'nocancer'\n",
    "        \n",
    "        # calculating true positive, false negative or false positive based on ground truth and predicted class\n",
    "        if predicted_class == 'cancer' and document_class == 'cancer':\n",
    "            tp = tp + 1\n",
    "        \n",
    "        if predicted_class == 'nocancer' and document_class == 'cancer':\n",
    "            fn = fn + 1 \n",
    "        \n",
    "        if predicted_class == 'cancer' and document_class == 'nocancer':\n",
    "            fp = fp + 1\n",
    "    \n",
    "    #Calculating precision, recall and f1score\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    f1score = 2*precision*recall/ (precision+recall)\n",
    "    \n",
    "    # Printing the results\n",
    "    print(\"Training set used   : \" + training_dataset_name)\n",
    "    print('Stop words excluded : ' + stop_word_exclusion_flag)\n",
    "    print('Binary version      : ' + binary_flag)\n",
    "    print('true positives      : ' + str(tp))\n",
    "    print('false negatives     : ' + str(fn))\n",
    "    print('false positives     : ' + str(fp))   \n",
    "    print (\"Precision is       : \" + str(round(precision,2)))\n",
    "    print (\"Recall is          : \" + str(round(recall,2)))\n",
    "    print (\"F-score is         : \" + str(round(f1score,2)))\n",
    "\n",
    "#Calling function to calculate the performance score    \n",
    "calculate_scores(testing_dataset,training_dataset )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
